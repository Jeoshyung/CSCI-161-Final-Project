{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY3m9duqwgCX"
   },
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avopLInWwgCf",
    "outputId": "c1d6d862-b852-4457-8aac-f41de7ab0536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.13/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.13/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: emoji in ./.venv/lib/python3.13/site-packages (2.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install nltk\n",
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbVZZ9mKwgCk",
    "outputId": "feb5a84d-34fa-4f18-af93-3ee62e234aad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Zapi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/Zapi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/Zapi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import ssl, certifi\n",
    "ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-u7eHX6IwgCo"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d5uEBgc2wgCp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/rkd761pn3351cms_dnnltxn40000gp/T/ipykernel_1577/3076581574.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  rappler_docs['date_published'] = pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "rappler_docs = pd.read_excel('data/rappler_old.xlsx')\n",
    "youtube_docs = pd.read_excel('data/youtube_old.xlsx')\n",
    "\n",
    "# Drop unnamed columns\n",
    "rappler_docs = rappler_docs.loc[:, ~rappler_docs.columns.str.contains(\"^Unnamed\")]\n",
    "youtube_docs = youtube_docs.loc[:, ~youtube_docs.columns.str.contains(\"^Unnamed\")]\n",
    "\n",
    "# Parse datetimes\n",
    "# Rappler: already has +08:00 offset, just parse and strip tz\n",
    "rappler_docs['date_published'] = pd.to_datetime(\n",
    "    rappler_docs['date_published'], errors='coerce'\n",
    ").dt.tz_localize(None)\n",
    "\n",
    "# YouTube: UTC ‚Üí Manila\n",
    "youtube_docs['date_published'] = pd.to_datetime(\n",
    "    youtube_docs['date_published'], errors='coerce', utc=True\n",
    ").dt.tz_convert('Asia/Manila').dt.tz_localize(None)\n",
    "\n",
    "# Add missing columns to Rappler\n",
    "rappler_docs['like_count'] = pd.NA\n",
    "rappler_docs['reply_parent_id'] = pd.NA\n",
    "\n",
    "# Add source\n",
    "rappler_docs['source'] = 'rappler'\n",
    "youtube_docs['source'] = 'youtube'\n",
    "\n",
    "# Reorder columns consistently\n",
    "column_order = [\n",
    "    \"title\", \"link\", \"date_published\", \"text\",\n",
    "    \"like_count\", \"reply_parent_id\", \"source\"\n",
    "]\n",
    "\n",
    "rappler_docs = rappler_docs[column_order]\n",
    "youtube_docs = youtube_docs[column_order]\n",
    "\n",
    "# Combine datasets\n",
    "corpus = pd.concat([rappler_docs, youtube_docs], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jzo67tt1wgCr"
   },
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImJYVCGXwgCs"
   },
   "source": [
    "## Load stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jC0ujkewgCu",
    "outputId": "53d9f9bb-1b53-46f2-f021-17bd6c5fb6a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Zapi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pandas.errors import EmptyDataError\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    BASIC_STOPWORDS = list(\n",
    "        pd.read_csv('basic_stopwords.txt', header=None).values.flatten()\n",
    "    )\n",
    "except (FileNotFoundError, EmptyDataError):\n",
    "    BASIC_STOPWORDS = []\n",
    "\n",
    "try:\n",
    "    DOMAIN_STOPWORDS = list(\n",
    "        pd.read_csv('domain_stopwords.txt', header=None).values.flatten()\n",
    "    )\n",
    "except (FileNotFoundError, EmptyDataError):\n",
    "    DOMAIN_STOPWORDS = []\n",
    "\n",
    "EN_STOPWORDS_LIST = stopwords.words('english')\n",
    "\n",
    "EXTRA_STOPWORDS = [\n",
    "    \"ako\",\"ikaw\",\"siya\",\"kami\",\"tayo\",\"kayo\",\"sila\",\n",
    "    \"ko\",\"mo\",\"niya\",\"natin\",\"namin\",\"nila\",\"kanila\",\"atin\",\"amin\",\n",
    "    \"ang\",\"ng\",\"sa\",\"kay\",\"kina\",\"para\",\"mula\",\"galing\",\"ayon\",\n",
    "    \"dahil\",\"kung\",\"kapag\",\"bago\",\"hanggang\",\"habang\",\"pagkatapos\",\n",
    "    \"kaya\",\"pero\",\"ngunit\",\"subalit\",\"kahit\",\"kasi\",\"sapagkat\",\n",
    "    \"ito\",\"iyan\",\"iyon\",\"doon\",\"dito\",\"dyan\",\"diyan\",\"ngayon\",\"noon\",\n",
    "    \"mamaya\",\"kanina\",\"bukas\",\"kahapon\",\"palagi\",\"lagi\",\"minsan\",\n",
    "    \"madalas\",\"halos\",\"lamang\",\"lang\",\"na\",\"ay\",\"din\",\"rin\",\"daw\",\"raw\",\n",
    "    \"pa\",\"naman\",\"nga\",\"pala\",\"yata\",\"dapat\",\"hindi\",\"oo\",\"opo\",\"huwag\",\n",
    "    \"wala\",\"may\",\"meron\",\"saan\",\"kailan\",\"paano\",\"ano\",\"bakit\",\"sino\",\"alin\",\n",
    "    \"lahat\",\"iba\",\"ibang\",\"pareho\",\"ganito\",\"ganyan\",\"ganun\",\"ganoon\",\"gayunman\",\n",
    "    \"yan\", \"lahat\", \"walang\", \"pa\", \"ka\", \"ni\", \"po\", \"si\", \"lng\", \"nyo\", \"mga\", \"yung\", \"ba\", \"di\",\n",
    "    \"nya\", \"pag\", \"nya\", \"yang\", \"eh\", \"mag\", \"yan\", \"puro\", \"mag\",\n",
    "]\n",
    "\n",
    "# BASIC_STOPWORDS = BASIC_STOPWORDS + EXTRA_STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP4g3YiJwgCv"
   },
   "source": [
    "## Clean corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4s2flYNPwgCw"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import emoji\n",
    "\n",
    "\n",
    "def clean_corpus(corpus, text_column='text'):\n",
    "  '''\n",
    "  Clean the text data in the specified column of the DataFrame.\n",
    "  '''\n",
    "  cleaned_corpus = corpus.copy()\n",
    "\n",
    "  # Force text_column as string\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus[text_column].astype(str)\n",
    "\n",
    "  # Transform into lowercase\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].str.lower()\n",
    "\n",
    "  # Remove usernames, non-alphanumeric characters, and links\n",
    "  # docs['cleaned_text'] = docs['cleaned_text'].str.replace(r'(@[A-Za-z0-9_]+)|([^A-Za-z0-9_ \\t])|(\\w+:\\/\\/\\S+)', '')\n",
    "\n",
    "  # # Lemmatize (by default, lemmatize nouns)\n",
    "  # # Other options:\n",
    "  # #   'v' for verbs\n",
    "  # #   'a' for adjectives\n",
    "  # #   'r' for adverbs\n",
    "  # #   's' for satellites adjectives (adjectives that appear after verbs)\n",
    "  # lemmatizer = WordNetLemmatizer()\n",
    "  # cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "  #   lambda text: ' '.join(\n",
    "  #     [lemmatizer.lemmatize(word, pos='n') for word in str(text).split()]\n",
    "  #   )\n",
    "  # )\n",
    "\n",
    "  # # Stemmer\n",
    "  # stemmer = PorterStemmer()\n",
    "  # cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "  #     lambda text: ' '.join(\n",
    "  #       [stemmer.stem(word) for word in str(text).split()]\n",
    "  #     )\n",
    "  # )\n",
    "\n",
    "  # Remove non-alphanumeric characters FIRST\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].str.replace(r'\\W', ' ', regex=True)\n",
    "\n",
    "  # Remove numbers\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].str.replace(r'\\d+', ' ', regex=True)\n",
    "\n",
    "  # Remove emojis using emoji library\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "    lambda text: ' '.join(\n",
    "      emoji.replace_emoji(text, replace=' ').split()\n",
    "    )\n",
    "  )\n",
    "\n",
    "  # Remove trailing and leading whitespaces\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].str.strip()\n",
    "\n",
    "  # Remove NLTK stopwords\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "    lambda text: ' '.join(\n",
    "      [\n",
    "        word for word in text.split() if word not in EN_STOPWORDS_LIST\n",
    "      ]\n",
    "    )\n",
    "  )\n",
    "\n",
    "  # Remove basic stopwords\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "    lambda text: ' '.join(\n",
    "      [word for word in text.split() if word not in BASIC_STOPWORDS]\n",
    "    )\n",
    "  )\n",
    "\n",
    "  # Remove domain stopwords\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "    lambda text: ' '.join(\n",
    "      [word for word in text.split() if word not in DOMAIN_STOPWORDS]\n",
    "    )\n",
    "  )\n",
    "\n",
    "  # Remove extra stopwords\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "    lambda text: ' '.join(\n",
    "      [word for word in text.split() if word not in EXTRA_STOPWORDS]\n",
    "    )\n",
    "  )\n",
    "\n",
    "  # Remove trailing and leading whitespaces (final cleanup)\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].str.strip()\n",
    "\n",
    "  # Remove NaN values\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].replace(np.nan, '', regex=True)\n",
    "\n",
    "  return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "c6jAUu29wgCy",
    "outputId": "8a377d07-2429-4361-efa2-1dadac43c19c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>date_published</th>\n",
       "      <th>text</th>\n",
       "      <th>like_count</th>\n",
       "      <th>reply_parent_id</th>\n",
       "      <th>source</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Rear View] Is Marcos looking for his Napoles?</td>\n",
       "      <td>2025-08-22T16:00:00+08:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>President Ferdinand Marcos Jr. is doubling dow...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rappler</td>\n",
       "      <td>president ferdinand marcos jr doubling pledge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fixing the flood problem: What's in it for Ram...</td>\n",
       "      <td>2025-08-22T14:49:01+08:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>MANILA, Philippines ‚Äì Filipino billionaire Ram...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rappler</td>\n",
       "      <td>manila philippines filipino billionaire ramon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gardiola clan's DPWH deals hit billions after ...</td>\n",
       "      <td>2025-08-22T12:00:00+08:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Two construction firms owned by Construction W...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rappler</td>\n",
       "      <td>two construction firms owned construction work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[In This Economy] The hypocrisy in Marcos‚Äô new...</td>\n",
       "      <td>2025-08-22T10:44:11+08:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>President Ferdinand Marcos Jr. seems to be spe...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rappler</td>\n",
       "      <td>president ferdinand marcos jr seems spending i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which Bulacan towns got biggest slices of DPWH...</td>\n",
       "      <td>2025-08-22T08:00:00+08:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>With every typhoon or heavy downpour, large pa...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rappler</td>\n",
       "      <td>every typhoon heavy downpour large parts bulac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7859</th>\n",
       "      <td>Dahil after the typhoon wala na ang ebidensiya.</td>\n",
       "      <td>https://www.youtube.com/watch?v=QHKjGHbj-Gc&amp;lc...</td>\n",
       "      <td>2025-08-11 14:29:49</td>\n",
       "      <td>Dahil after the typhoon wala na ang ebidensiya.</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youtube</td>\n",
       "      <td>typhoon ebidensiya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7860</th>\n",
       "      <td>Yabang mo kasi! Inuuna nyo impeachment sira ulo!</td>\n",
       "      <td>https://www.youtube.com/watch?v=QHKjGHbj-Gc&amp;lc...</td>\n",
       "      <td>2025-08-11 14:28:26</td>\n",
       "      <td>Yabang mo kasi! Inuuna nyo impeachment sira ulo!</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youtube</td>\n",
       "      <td>yabang inuuna impeachment sira ulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7861</th>\n",
       "      <td>Gnyan klkaran ng kurakot s Dpwh ,lhat nyan my ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=QHKjGHbj-Gc&amp;lc...</td>\n",
       "      <td>2025-08-11 14:27:42</td>\n",
       "      <td>Gnyan klkaran ng kurakot s Dpwh ,lhat nyan my ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youtube</td>\n",
       "      <td>gnyan klkaran kurakot dpwh lhat nyan lgay frm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7862</th>\n",
       "      <td>Magic ni Sec. Bonoan yan alam na alam nya laha...</td>\n",
       "      <td>https://www.youtube.com/watch?v=QHKjGHbj-Gc&amp;lc...</td>\n",
       "      <td>2025-08-11 14:17:28</td>\n",
       "      <td>Magic ni Sec. Bonoan yan alam na alam nya laha...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youtube</td>\n",
       "      <td>magic sec bonoan alam alam sinong politiko sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7863</th>\n",
       "      <td>Kamara yan d muba alam sinsbi ni mayor magalon...</td>\n",
       "      <td>https://www.youtube.com/watch?v=QHKjGHbj-Gc&amp;lc...</td>\n",
       "      <td>2025-08-11 16:56:24</td>\n",
       "      <td>Kamara yan d muba alam sinsbi ni mayor magalon...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ugz2yhnhyLEQN1c2xO54AaABAg</td>\n",
       "      <td>youtube</td>\n",
       "      <td>kamara muba alam sinsbi mayor magalong bawat d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7864 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0        [Rear View] Is Marcos looking for his Napoles?   \n",
       "1     Fixing the flood problem: What's in it for Ram...   \n",
       "2     Gardiola clan's DPWH deals hit billions after ...   \n",
       "3     [In This Economy] The hypocrisy in Marcos‚Äô new...   \n",
       "4     Which Bulacan towns got biggest slices of DPWH...   \n",
       "...                                                 ...   \n",
       "7859    Dahil after the typhoon wala na ang ebidensiya.   \n",
       "7860   Yabang mo kasi! Inuuna nyo impeachment sira ulo!   \n",
       "7861  Gnyan klkaran ng kurakot s Dpwh ,lhat nyan my ...   \n",
       "7862  Magic ni Sec. Bonoan yan alam na alam nya laha...   \n",
       "7863  Kamara yan d muba alam sinsbi ni mayor magalon...   \n",
       "\n",
       "                                                   link      date_published  \\\n",
       "0                             2025-08-22T16:00:00+08:00                 NaT   \n",
       "1                             2025-08-22T14:49:01+08:00                 NaT   \n",
       "2                             2025-08-22T12:00:00+08:00                 NaT   \n",
       "3                             2025-08-22T10:44:11+08:00                 NaT   \n",
       "4                             2025-08-22T08:00:00+08:00                 NaT   \n",
       "...                                                 ...                 ...   \n",
       "7859  https://www.youtube.com/watch?v=QHKjGHbj-Gc&lc... 2025-08-11 14:29:49   \n",
       "7860  https://www.youtube.com/watch?v=QHKjGHbj-Gc&lc... 2025-08-11 14:28:26   \n",
       "7861  https://www.youtube.com/watch?v=QHKjGHbj-Gc&lc... 2025-08-11 14:27:42   \n",
       "7862  https://www.youtube.com/watch?v=QHKjGHbj-Gc&lc... 2025-08-11 14:17:28   \n",
       "7863  https://www.youtube.com/watch?v=QHKjGHbj-Gc&lc... 2025-08-11 16:56:24   \n",
       "\n",
       "                                                   text like_count  \\\n",
       "0     President Ferdinand Marcos Jr. is doubling dow...       <NA>   \n",
       "1     MANILA, Philippines ‚Äì Filipino billionaire Ram...       <NA>   \n",
       "2     Two construction firms owned by Construction W...       <NA>   \n",
       "3     President Ferdinand Marcos Jr. seems to be spe...       <NA>   \n",
       "4     With every typhoon or heavy downpour, large pa...       <NA>   \n",
       "...                                                 ...        ...   \n",
       "7859    Dahil after the typhoon wala na ang ebidensiya.          2   \n",
       "7860   Yabang mo kasi! Inuuna nyo impeachment sira ulo!          0   \n",
       "7861  Gnyan klkaran ng kurakot s Dpwh ,lhat nyan my ...          0   \n",
       "7862  Magic ni Sec. Bonoan yan alam na alam nya laha...          1   \n",
       "7863  Kamara yan d muba alam sinsbi ni mayor magalon...          0   \n",
       "\n",
       "                 reply_parent_id   source  \\\n",
       "0                            NaN  rappler   \n",
       "1                            NaN  rappler   \n",
       "2                            NaN  rappler   \n",
       "3                            NaN  rappler   \n",
       "4                            NaN  rappler   \n",
       "...                          ...      ...   \n",
       "7859                         NaN  youtube   \n",
       "7860                         NaN  youtube   \n",
       "7861                         NaN  youtube   \n",
       "7862                         NaN  youtube   \n",
       "7863  Ugz2yhnhyLEQN1c2xO54AaABAg  youtube   \n",
       "\n",
       "                                           cleaned_text  \n",
       "0     president ferdinand marcos jr doubling pledge ...  \n",
       "1     manila philippines filipino billionaire ramon ...  \n",
       "2     two construction firms owned construction work...  \n",
       "3     president ferdinand marcos jr seems spending i...  \n",
       "4     every typhoon heavy downpour large parts bulac...  \n",
       "...                                                 ...  \n",
       "7859                                 typhoon ebidensiya  \n",
       "7860                 yabang inuuna impeachment sira ulo  \n",
       "7861  gnyan klkaran kurakot dpwh lhat nyan lgay frm ...  \n",
       "7862  magic sec bonoan alam alam sinong politiko sin...  \n",
       "7863  kamara muba alam sinsbi mayor magalong bawat d...  \n",
       "\n",
       "[7864 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = clean_corpus(corpus, text_column='text')\n",
    "cleaned_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVdYEd3dwgCz"
   },
   "source": [
    "### Cleaning emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDNAnCcAwgC0",
    "outputId": "43d13ddb-54f8-4374-d030-2f8391ef05e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait ko si dugong mag salita na JOKE LNGüòÅ‚ò∫Ô∏è<br>Kayu naman naniniwla agadüòÇ\n",
      "Wait ko si dugong mag salita na JOKE LNG EMOJI EMOJI<br>Kayu naman naniniwla agad EMOJI\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"hello world @helloWorld üòÖ\"\n",
    "sample_sentence_2 = \"Wait ko si dugong mag salita na JOKE LNGüòÅ‚ò∫Ô∏è<br>Kayu naman naniniwla agadüòÇ\"\n",
    "\n",
    "# Clean emojis\n",
    "sample_sentence_without_emoji = emoji.replace_emoji(\n",
    "  sample_sentence, replace=' EMOJI')\n",
    "sample_sentence_2_without_emoji = emoji.replace_emoji(\n",
    "  sample_sentence_2, replace=' EMOJI')\n",
    "\n",
    "print(sample_sentence_2)\n",
    "print(sample_sentence_2_without_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e5c11fa7"
   },
   "outputs": [],
   "source": [
    "cleaned_corpus.to_excel('data/cleaned_corpus.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
