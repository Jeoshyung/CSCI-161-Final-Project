{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY3m9duqwgCX"
   },
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avopLInWwgCf",
    "outputId": "c1d6d862-b852-4457-8aac-f41de7ab0536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.13/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: click, nltk\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [nltk][32m1/2\u001b[0m [nltk]\n",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.3.0 nltk-3.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting emoji\n",
      "  Using cached emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Using cached emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install nltk\n",
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbVZZ9mKwgCk",
    "outputId": "feb5a84d-34fa-4f18-af93-3ee62e234aad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Zapi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/Zapi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/Zapi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import ssl, certifi\n",
    "ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-u7eHX6IwgCo"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d5uEBgc2wgCp"
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "rappler_docs = pd.read_excel('data/rappler_current.xlsx')\n",
    "youtube_docs = pd.read_excel('data/youtube_current.xlsx')\n",
    "\n",
    "# Drop unnamed columns\n",
    "rappler_docs = rappler_docs.loc[:, ~rappler_docs.columns.str.contains(\"^Unnamed\")]\n",
    "youtube_docs = youtube_docs.loc[:, ~youtube_docs.columns.str.contains(\"^Unnamed\")]\n",
    "\n",
    "# Parse datetimes\n",
    "# Rappler: already has +08:00 offset, just parse and strip tz\n",
    "rappler_docs['date_published'] = pd.to_datetime(\n",
    "    rappler_docs['date_published'], errors='coerce'\n",
    ").dt.tz_localize(None)\n",
    "\n",
    "# YouTube: UTC ‚Üí Manila\n",
    "youtube_docs['date_published'] = pd.to_datetime(\n",
    "    youtube_docs['date_published'], errors='coerce', utc=True\n",
    ").dt.tz_convert('Asia/Manila').dt.tz_localize(None)\n",
    "\n",
    "# Add missing columns to Rappler\n",
    "rappler_docs['like_count'] = pd.NA\n",
    "rappler_docs['reply_parent_id'] = pd.NA\n",
    "\n",
    "# Add source\n",
    "rappler_docs['source'] = 'rappler'\n",
    "youtube_docs['source'] = 'youtube'\n",
    "\n",
    "# Reorder columns consistently\n",
    "column_order = [\n",
    "    \"title\", \"link\", \"date_published\", \"text\",\n",
    "    \"like_count\", \"reply_parent_id\", \"source\"\n",
    "]\n",
    "\n",
    "rappler_docs = rappler_docs[column_order]\n",
    "youtube_docs = youtube_docs[column_order]\n",
    "\n",
    "# Combine datasets\n",
    "corpus = pd.concat([rappler_docs, youtube_docs], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jzo67tt1wgCr"
   },
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImJYVCGXwgCs"
   },
   "source": [
    "## Load stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jC0ujkewgCu",
    "outputId": "53d9f9bb-1b53-46f2-f021-17bd6c5fb6a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Zapi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pandas.errors import EmptyDataError\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    BASIC_STOPWORDS = list(\n",
    "        pd.read_csv('basic_stopwords.txt', header=None).values.flatten()\n",
    "    )\n",
    "except (FileNotFoundError, EmptyDataError):\n",
    "    BASIC_STOPWORDS = []\n",
    "\n",
    "try:\n",
    "    DOMAIN_STOPWORDS = list(\n",
    "        pd.read_csv('domain_stopwords.txt', header=None).values.flatten()\n",
    "    )\n",
    "except (FileNotFoundError, EmptyDataError):\n",
    "    DOMAIN_STOPWORDS = []\n",
    "\n",
    "EN_STOPWORDS_LIST = stopwords.words('english')\n",
    "\n",
    "EXTRA_STOPWORDS = [\n",
    "    \"ako\",\"ikaw\",\"siya\",\"kami\",\"tayo\",\"kayo\",\"sila\",\n",
    "    \"ko\",\"mo\",\"niya\",\"natin\",\"namin\",\"nila\",\"kanila\",\"atin\",\"amin\",\n",
    "    \"ang\",\"ng\",\"sa\",\"kay\",\"kina\",\"para\",\"mula\",\"galing\",\"ayon\",\n",
    "    \"dahil\",\"kung\",\"kapag\",\"bago\",\"hanggang\",\"habang\",\"pagkatapos\",\n",
    "    \"kaya\",\"pero\",\"ngunit\",\"subalit\",\"kahit\",\"kasi\",\"sapagkat\",\n",
    "    \"ito\",\"iyan\",\"iyon\",\"doon\",\"dito\",\"dyan\",\"diyan\",\"ngayon\",\"noon\",\n",
    "    \"mamaya\",\"kanina\",\"bukas\",\"kahapon\",\"palagi\",\"lagi\",\"minsan\",\n",
    "    \"madalas\",\"halos\",\"lamang\",\"lang\",\"na\",\"ay\",\"din\",\"rin\",\"daw\",\"raw\",\n",
    "    \"pa\",\"naman\",\"nga\",\"pala\",\"yata\",\"dapat\",\"hindi\",\"oo\",\"opo\",\"huwag\",\n",
    "    \"wala\",\"may\",\"meron\",\"saan\",\"kailan\",\"paano\",\"ano\",\"bakit\",\"sino\",\"alin\",\n",
    "    \"lahat\",\"iba\",\"ibang\",\"pareho\",\"ganito\",\"ganyan\",\"ganun\",\"ganoon\",\"gayunman\",\n",
    "    \"yan\", \"lahat\", \"walang\", \"pa\", \"ka\", \"ni\", \"po\", \"si\", \"lng\", \"nyo\", \"mga\", \"yung\", \"ba\", \"di\",\n",
    "    \"nya\", \"pag\", \"nya\", \"yang\", \"eh\", \"mag\", \"yan\", \"puro\", \"mag\",\n",
    "]\n",
    "\n",
    "# BASIC_STOPWORDS = BASIC_STOPWORDS + EXTRA_STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP4g3YiJwgCv"
   },
   "source": [
    "## Clean corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4s2flYNPwgCw"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import emoji\n",
    "\n",
    "\n",
    "def clean_corpus(corpus, text_column='text'):\n",
    "  '''\n",
    "  Clean the text data in the specified column of the DataFrame.\n",
    "  '''\n",
    "  cleaned_corpus = corpus.copy()\n",
    "\n",
    "  # Force text_column as string\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus[text_column].astype(str)\n",
    "\n",
    "  # Transform into lowercase\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].str.lower()\n",
    "\n",
    "  # Remove usernames, non-alphanumeric characters, and links\n",
    "  # docs['cleaned_text'] = docs['cleaned_text'].str.replace(r'(@[A-Za-z0-9_]+)|([^A-Za-z0-9_ \\t])|(\\w+:\\/\\/\\S+)', '')\n",
    "\n",
    "  # # Lemmatize (by default, lemmatize nouns)\n",
    "  # # Other options:\n",
    "  # #   'v' for verbs\n",
    "  # #   'a' for adjectives\n",
    "  # #   'r' for adverbs\n",
    "  # #   's' for satellites adjectives (adjectives that appear after verbs)\n",
    "  # lemmatizer = WordNetLemmatizer()\n",
    "  # cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "  #   lambda text: ' '.join(\n",
    "  #     [lemmatizer.lemmatize(word, pos='n') for word in str(text).split()]\n",
    "  #   )\n",
    "  # )\n",
    "\n",
    "  # # Stemmer\n",
    "  # stemmer = PorterStemmer()\n",
    "  # cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "  #     lambda text: ' '.join(\n",
    "  #       [stemmer.stem(word) for word in str(text).split()]\n",
    "  #     )\n",
    "  # )\n",
    "\n",
    "  # Remove non-alphanumeric characters FIRST\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].str.replace(r'\\W', ' ', regex=True)\n",
    "\n",
    "  # Remove numbers\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].str.replace(r'\\d+', ' ', regex=True)\n",
    "\n",
    "  # Remove emojis using emoji library\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "    lambda text: ' '.join(\n",
    "      emoji.replace_emoji(text, replace=' ').split()\n",
    "    )\n",
    "  )\n",
    "\n",
    "  # Remove trailing and leading whitespaces\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].str.strip()\n",
    "\n",
    "  # Remove NLTK stopwords\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "    lambda text: ' '.join(\n",
    "      [\n",
    "        word for word in text.split() if word not in EN_STOPWORDS_LIST\n",
    "      ]\n",
    "    )\n",
    "  )\n",
    "\n",
    "  # Remove basic stopwords\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "    lambda text: ' '.join(\n",
    "      [word for word in text.split() if word not in BASIC_STOPWORDS]\n",
    "    )\n",
    "  )\n",
    "\n",
    "  # Remove domain stopwords\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "    lambda text: ' '.join(\n",
    "      [word for word in text.split() if word not in DOMAIN_STOPWORDS]\n",
    "    )\n",
    "  )\n",
    "\n",
    "  # Remove extra stopwords\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].apply(\n",
    "    lambda text: ' '.join(\n",
    "      [word for word in text.split() if word not in EXTRA_STOPWORDS]\n",
    "    )\n",
    "  )\n",
    "\n",
    "  # Remove trailing and leading whitespaces (final cleanup)\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].str.strip()\n",
    "\n",
    "  # Remove NaN values\n",
    "  cleaned_corpus['cleaned_text'] = cleaned_corpus['cleaned_text'].replace(np.nan, '', regex=True)\n",
    "\n",
    "  return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "c6jAUu29wgCy",
    "outputId": "8a377d07-2429-4361-efa2-1dadac43c19c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>date_published</th>\n",
       "      <th>text</th>\n",
       "      <th>like_count</th>\n",
       "      <th>reply_parent_id</th>\n",
       "      <th>source</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.rappler.com/philippines/dpwh-manue...</td>\n",
       "      <td>2025-08-31 17:02:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rappler</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.rappler.com/philippines/dpwh-suspe...</td>\n",
       "      <td>2025-09-03 11:20:56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rappler</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.rappler.com/philippines/flood-cont...</td>\n",
       "      <td>2025-09-04 15:01:13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rappler</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.rappler.com/philippines/coa-holds-...</td>\n",
       "      <td>2025-09-09 10:47:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rappler</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.rappler.com/philippines/visayas/ma...</td>\n",
       "      <td>2025-08-14 15:36:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rappler</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>Do u really have to investigate since it&amp;#39;s...</td>\n",
       "      <td>https://www.youtube.com/watch?v=SpYDbT-PHeA&amp;lc...</td>\n",
       "      <td>2025-11-14 22:08:07</td>\n",
       "      <td>Do u really have to investigate since it's cle...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youtube</td>\n",
       "      <td>u really investigate since clear ghost project...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>They are govt officials that needs appearance ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=SpYDbT-PHeA&amp;lc...</td>\n",
       "      <td>2025-11-14 22:06:20</td>\n",
       "      <td>They are govt officials that needs appearance ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youtube</td>\n",
       "      <td>govt officials needs appearance investigtion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>Lagot kayong mga kurakot Kay sir sen general l...</td>\n",
       "      <td>https://www.youtube.com/watch?v=SpYDbT-PHeA&amp;lc...</td>\n",
       "      <td>2025-11-14 20:56:23</td>\n",
       "      <td>Lagot kayong mga kurakot Kay sir sen general l...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youtube</td>\n",
       "      <td>lagot kayong kurakot sir sen general lacson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>Dapat kasuhan na Sila nagaalburuto na Ang mamayan</td>\n",
       "      <td>https://www.youtube.com/watch?v=SpYDbT-PHeA&amp;lc...</td>\n",
       "      <td>2025-11-14 20:49:15</td>\n",
       "      <td>Dapat kasuhan na Sila nagaalburuto na Ang mamayan</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youtube</td>\n",
       "      <td>kasuhan nagaalburuto mamayan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>US News on Philippine affairs? AYOS AH</td>\n",
       "      <td>https://www.youtube.com/watch?v=SpYDbT-PHeA&amp;lc...</td>\n",
       "      <td>2025-11-14 18:28:50</td>\n",
       "      <td>US News on Philippine affairs? AYOS AH</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youtube</td>\n",
       "      <td>us news philippine affairs ayos ah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2150 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "2145  Do u really have to investigate since it&#39;s...   \n",
       "2146  They are govt officials that needs appearance ...   \n",
       "2147  Lagot kayong mga kurakot Kay sir sen general l...   \n",
       "2148  Dapat kasuhan na Sila nagaalburuto na Ang mamayan   \n",
       "2149             US News on Philippine affairs? AYOS AH   \n",
       "\n",
       "                                                   link      date_published  \\\n",
       "0     https://www.rappler.com/philippines/dpwh-manue... 2025-08-31 17:02:21   \n",
       "1     https://www.rappler.com/philippines/dpwh-suspe... 2025-09-03 11:20:56   \n",
       "2     https://www.rappler.com/philippines/flood-cont... 2025-09-04 15:01:13   \n",
       "3     https://www.rappler.com/philippines/coa-holds-... 2025-09-09 10:47:37   \n",
       "4     https://www.rappler.com/philippines/visayas/ma... 2025-08-14 15:36:45   \n",
       "...                                                 ...                 ...   \n",
       "2145  https://www.youtube.com/watch?v=SpYDbT-PHeA&lc... 2025-11-14 22:08:07   \n",
       "2146  https://www.youtube.com/watch?v=SpYDbT-PHeA&lc... 2025-11-14 22:06:20   \n",
       "2147  https://www.youtube.com/watch?v=SpYDbT-PHeA&lc... 2025-11-14 20:56:23   \n",
       "2148  https://www.youtube.com/watch?v=SpYDbT-PHeA&lc... 2025-11-14 20:49:15   \n",
       "2149  https://www.youtube.com/watch?v=SpYDbT-PHeA&lc... 2025-11-14 18:28:50   \n",
       "\n",
       "                                                   text like_count  \\\n",
       "0                                                   NaN       <NA>   \n",
       "1                                                   NaN       <NA>   \n",
       "2                                                   NaN       <NA>   \n",
       "3                                                   NaN       <NA>   \n",
       "4                                                   NaN       <NA>   \n",
       "...                                                 ...        ...   \n",
       "2145  Do u really have to investigate since it's cle...          0   \n",
       "2146  They are govt officials that needs appearance ...          0   \n",
       "2147  Lagot kayong mga kurakot Kay sir sen general l...          1   \n",
       "2148  Dapat kasuhan na Sila nagaalburuto na Ang mamayan          3   \n",
       "2149             US News on Philippine affairs? AYOS AH          1   \n",
       "\n",
       "     reply_parent_id   source  \\\n",
       "0                NaN  rappler   \n",
       "1                NaN  rappler   \n",
       "2                NaN  rappler   \n",
       "3                NaN  rappler   \n",
       "4                NaN  rappler   \n",
       "...              ...      ...   \n",
       "2145             NaN  youtube   \n",
       "2146             NaN  youtube   \n",
       "2147             NaN  youtube   \n",
       "2148             NaN  youtube   \n",
       "2149             NaN  youtube   \n",
       "\n",
       "                                           cleaned_text  \n",
       "0                                                   nan  \n",
       "1                                                   nan  \n",
       "2                                                   nan  \n",
       "3                                                   nan  \n",
       "4                                                   nan  \n",
       "...                                                 ...  \n",
       "2145  u really investigate since clear ghost project...  \n",
       "2146       govt officials needs appearance investigtion  \n",
       "2147        lagot kayong kurakot sir sen general lacson  \n",
       "2148                       kasuhan nagaalburuto mamayan  \n",
       "2149                 us news philippine affairs ayos ah  \n",
       "\n",
       "[2150 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = clean_corpus(corpus, text_column='text')\n",
    "cleaned_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVdYEd3dwgCz"
   },
   "source": [
    "### Cleaning emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDNAnCcAwgC0",
    "outputId": "43d13ddb-54f8-4374-d030-2f8391ef05e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait ko si dugong mag salita na JOKE LNGüòÅ‚ò∫Ô∏è<br>Kayu naman naniniwla agadüòÇ\n",
      "Wait ko si dugong mag salita na JOKE LNG EMOJI EMOJI<br>Kayu naman naniniwla agad EMOJI\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"hello world @helloWorld üòÖ\"\n",
    "sample_sentence_2 = \"Wait ko si dugong mag salita na JOKE LNGüòÅ‚ò∫Ô∏è<br>Kayu naman naniniwla agadüòÇ\"\n",
    "\n",
    "# Clean emojis\n",
    "sample_sentence_without_emoji = emoji.replace_emoji(\n",
    "  sample_sentence, replace=' EMOJI')\n",
    "sample_sentence_2_without_emoji = emoji.replace_emoji(\n",
    "  sample_sentence_2, replace=' EMOJI')\n",
    "\n",
    "print(sample_sentence_2)\n",
    "print(sample_sentence_2_without_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e5c11fa7"
   },
   "outputs": [],
   "source": [
    "cleaned_corpus.to_excel('data/cleaned_corpus.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
